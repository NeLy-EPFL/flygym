{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qSwjm60DA5t6"
      },
      "source": [
        "## Reinforcement learning"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "uBAuXiu8Bk9m"
      },
      "source": [
        "## Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQGkf_j3T90g",
        "outputId": "fffddd3a-266d-4af5-b24e-e07b0da0c1e6"
      },
      "outputs": [],
      "source": [
        "#@title Install `stable-baseline3` on Colab\n",
        "\n",
        "!apt-get update && apt-get install swig cmake\n",
        "!pip install box2d-py\n",
        "!pip install gymnasium\n",
        "!pip install \"stable-baselines3[extra]>=2.0.0a4\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Iv0lrfXDBUhQ"
      },
      "outputs": [],
      "source": [
        "#@title Install `flygym` on Colab\n",
        "\n",
        "# This block is modified from dm_control's tutorial notebook\n",
        "# https://github.com/deepmind/dm_control/blob/main/tutorial.ipynb\n",
        "\n",
        "try:\n",
        "  import google.colab\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    import subprocess\n",
        "    if subprocess.run('nvidia-smi').returncode:\n",
        "        raise RuntimeError(\n",
        "            'Cannot communicate with GPU. '\n",
        "            'Make sure you are using a GPU Colab runtime. '\n",
        "            'Go to the Runtime menu and select Choose runtime type.')\n",
        "\n",
        "    print('Installing flygym')\n",
        "    !pip install -q --progress-bar=off 'flygym[mujoco] @ git+https://github.com/NeLy-EPFL/flygym.git'\n",
        "\n",
        "    # Configure dm_control to use the EGL rendering backend (requires GPU)\n",
        "    %env MUJOCO_GL=egl\n",
        "\n",
        "    print('Checking that the dm_control installation succeeded...')\n",
        "    try:\n",
        "        from dm_control import suite\n",
        "        env = suite.load('cartpole', 'swingup')\n",
        "        pixels = env.physics.render()\n",
        "    except Exception as e:\n",
        "        raise e from RuntimeError(\n",
        "            'Something went wrong during dm_control installation. Check the shell '\n",
        "            'output above for more information.\\n'\n",
        "            'If using a hosted Colab runtime, make sure you enable GPU acceleration '\n",
        "            'by going to the Runtime menu and selecting \"Choose runtime type\".')\n",
        "    else:\n",
        "        del pixels, suite\n",
        "\n",
        "    print('Checking that the flygym installation succeeded...')\n",
        "    try:\n",
        "        import flygym\n",
        "        from flygym import envs\n",
        "    except Exception as e:\n",
        "        raise e from RuntimeError(\n",
        "            'Something went wrong during flygym installation. Check the shell '\n",
        "            'output above for more information.\\n')\n",
        "    else:\n",
        "        del envs, flygym\n",
        "else:\n",
        "    print('Skipping - not on Colab')\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "yMpfR0ZVBwZq"
      },
      "source": [
        "## Demo: Cartpole\n",
        "\n",
        "We will demonstrate the use of reinforcement learning in training a controller for Cartpole: a toy environment where you try to balance a vertical pole on a cart by moving the cart left and right.\n",
        "\n",
        "Cartpole is a predefined Gym, which makes it very easy to initialize. In the following code, the `gym.make` function creates a Gym environment that has been registered to `Gym`; it is equivalent to our `nmf = NeuroMechFlyMuJoCo(...)` call. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wEXloVA_BvxX",
        "outputId": "537e3d20-e30a-4869-8e1b-765d9a3d2337"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "\n",
        "cartpole_env = gym.make('CartPole-v1', render_mode='rgb_array')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "DvMZXF5FCbuj"
      },
      "source": [
        "Next, we initialize a model using `stable-baselines3`. This, once again, is a one-liner:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgAK9NIQCbgB",
        "outputId": "4f690940-b69a-4f61-a2ce-1765edb3156a"
      },
      "outputs": [],
      "source": [
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.ppo import MlpPolicy\n",
        "\n",
        "cartpole_model = PPO(MlpPolicy, cartpole_env, verbose=1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4svLRgPqDX2z"
      },
      "source": [
        "We can now evaluate the untrained random policy for the task. Note that we wrap the environment around the `Monitor` class, which is used to keep track of information like episode reward."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ylwSq6cCDewc",
        "outputId": "38c99582-e942-4c93-9216-09039a2db9fd"
      },
      "outputs": [],
      "source": [
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "\n",
        "mean_reward, std_reward = evaluate_policy(cartpole_model,\n",
        "                                          Monitor(cartpole_env),\n",
        "                                          n_eval_episodes=100)\n",
        "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2rorzhEHBcsU"
      },
      "source": [
        "Remember the reward is simply the number of timesteps where the controller mangaged to keep the pole upright. The result is not very impressive.\n",
        "\n",
        "Now, we can train the model for 10,000 iterations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "cf5b496ca065444cb7b872cf8892e0ea",
            "9b9597d40b99416486f21e6b8d0941d1"
          ]
        },
        "id": "QDoywZQTEO0z",
        "outputId": "44b3b31e-b81e-4445-b52d-07781c187c17"
      },
      "outputs": [],
      "source": [
        "cartpole_model.learn(total_timesteps=10000, progress_bar=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bcIDV-jsBcdE"
      },
      "source": [
        "Let's reevaluate the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0TezzFR4EV6N",
        "outputId": "bc506a48-f0a0-4b36-b376-478347497d47"
      },
      "outputs": [],
      "source": [
        "mean_reward, std_reward = evaluate_policy(cartpole_model,\n",
        "                                          Monitor(cartpole_env),\n",
        "                                          n_eval_episodes=100)\n",
        "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "IdjE-19GEcIm"
      },
      "source": [
        "Better. Let's take a look at a video of a simulation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3b_FJBHEWmq",
        "outputId": "c4739183-f6a4-4357-915a-066df39f3eee"
      },
      "outputs": [],
      "source": [
        "obs = cartpole_env.reset()\n",
        "scenes = []\n",
        "for i in range(500):\n",
        "    action, _ = cartpole_model.predict(cartpole_env.state)\n",
        "    obs, reward, terminated, truncated, info = cartpole_env.step(action)\n",
        "    scenes.append(cartpole_env.render())\n",
        "    if terminated:\n",
        "        # stop early if the simulation terminates early because the pole fell\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "KsX0iFXFEWXq",
        "outputId": "b074742f-2f83-4439-ed1d-88d0eb70b59e"
      },
      "outputs": [],
      "source": [
        "import mediapy\n",
        "mediapy.show_video(scenes)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "nvtF10IVFZQ2"
      },
      "source": [
        "## Controlling NeuroMechFly with RL\n",
        "\n",
        "As discussed in the lecture, we will now try to control the stepping of each legs with reinforcement learning.\n",
        "\n",
        "First we need to write our own Gym environment that functions as a \"wrapper\" around the underlying the `NeuroMechFlyMuJoCo` object. You can achieve this by implementing a class inheriting from `gym.Env` with the actual `NeuroMechFlyMuJoCo` simulation saved as an attribute.\n",
        "\n",
        "There are three things to note here:\n",
        "1. For the gym environment to work with models in Stable Baselines 3, the observation and action spaces have to be arrays instead of dictionaries of arrays. We do this by concatenating the flattened arrays into a single array.\n",
        "2. Under `__init__`, you have to define the expected dimensions and bounds of the observation/action space, so the model knows what inputs/outputs are valid.\n",
        "3. The `step` method has to return five values: the observation, the reward, whether the simulation is terminated, whether the simulation is truncated, and some additional info. This is different from `NeuroMechFlyMuJoCo`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NweX2VHfGAiQ"
      },
      "outputs": [],
      "source": [
        "from gymnasium import spaces\n",
        "from flygym.envs.nmf_mujoco import MuJoCoParameters, NeuroMechFlyMuJoCo\n",
        "import numpy as np\n",
        "\n",
        "class MyNMF(gym.Env):\n",
        "    def __init__(self, **kwargs):\n",
        "        sim_params = MuJoCoParameters(timestep=1e-4, \n",
        "                                      render_mode=\"saved\", \n",
        "                                      render_playspeed=0.1, \n",
        "                                      render_camera='Animat/camera_left_top'\n",
        "                                    )\n",
        "        self.nmf = NeuroMechFlyMuJoCo(**kwargs)\n",
        "        num_dofs = len(self.nmf.actuated_joints)\n",
        "        bound = 0.5\n",
        "        self.action_space = spaces.Box(low=-bound, high=bound,\n",
        "                                       shape=(num_dofs,))\n",
        "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf,\n",
        "                                            shape=(num_dofs,))\n",
        "    \n",
        "    def _parse_obs(self, raw_obs):\n",
        "        features = [\n",
        "            raw_obs['joints'][:, 0].flatten(),\n",
        "            # raw_obs['fly'].flatten(),\n",
        "            # what else would you like to include?\n",
        "        ]\n",
        "        print(raw_obs['joints'].shape)\n",
        "        return np.concatenate(features, dtype=np.float32)\n",
        "    \n",
        "    def reset(self):\n",
        "        raw_obs, info = self.nmf.reset()\n",
        "        return self._parse_obs(raw_obs), info\n",
        "        \n",
        "    def step(self, action):\n",
        "        raw_obs, info = self.nmf.step({'joints': action})\n",
        "        obs = self._parse_obs(raw_obs)\n",
        "        joint_pos = raw_obs['joints'][0, :]\n",
        "        fly_pos = raw_obs['fly'][0, :]\n",
        "        reward = ...  # what is your reward function?\n",
        "        terminated = False\n",
        "        truncated = False\n",
        "        return obs, reward, terminated, truncated, info\n",
        "\n",
        "    def render(self):\n",
        "        return self.nmf.render()\n",
        "    \n",
        "    def close(self):\n",
        "        return self.nmf.close()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Og0HRn51GCpz"
      },
      "source": [
        "We can now train a agent on this environment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8qhx6G9BJ1PR"
      },
      "outputs": [],
      "source": [
        "from flygym.state import stretched_pose\n",
        "\n",
        "run_time = 0.5\n",
        "nmf_env_headless = MyNMF(init_pose=stretched_pose,\n",
        "                         actuated_joints=...)  # which DoFs would you use?\n",
        "nmf_model = PPO(MlpPolicy, nmf_env_headless, verbose=1)\n",
        "nmf_model.learn(total_timesteps=100_000, progress_bar=True)\n",
        "nmf_model.close()\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bYF3x3xYJ_dV"
      },
      "source": [
        "... and evaluate it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gBu3XHFgKB5o"
      },
      "outputs": [],
      "source": [
        "nmf_env_rendered = MyNMF(init_pose=stretched_pose,\n",
        "                         actuated_joints=...)\n",
        "obs, _ = nmf_env_rendered.reset()\n",
        "obs_list = []\n",
        "rew_list = []\n",
        "for i in range(int(run_time / nmf_env_rendered.nmf.timestep)):\n",
        "    action, _ = nmf_model.predict(obs)\n",
        "    obs, reward, terminated, truncated, info = nmf_env_rendered.step(action)\n",
        "    obs_list.append(obs)\n",
        "    rew_list.append(reward)\n",
        "    nmf_env_rendered.render()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "NYa2iPSzKWJ7"
      },
      "source": [
        "We can also visualize the results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rRRUOJCPKYDU"
      },
      "outputs": [],
      "source": [
        "nmf_env_rendered.nmf.save_video('filename.mp4')\n",
        "nmf_env_rendered.close()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9b9597d40b99416486f21e6b8d0941d1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf5b496ca065444cb7b872cf8892e0ea": {
          "model_module": "@jupyter-widgets/output",
          "model_module_version": "1.0.0",
          "model_name": "OutputModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_9b9597d40b99416486f21e6b8d0941d1",
            "msg_id": "",
            "outputs": [
              {
                "data": {
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080\"> 100%</span> <span style=\"color: #729c1f; text-decoration-color: #729c1f\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #008000; text-decoration-color: #008000\">10,225/10,000 </span> [ <span style=\"color: #808000; text-decoration-color: #808000\">0:00:24</span> &lt; <span style=\"color: #008080; text-decoration-color: #008080\">0:00:00</span> , <span style=\"color: #800000; text-decoration-color: #800000\">406 it/s</span> ]\n</pre>\n",
                  "text/plain": "\u001b[35m 100%\u001b[0m \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10,225/10,000 \u001b[0m [ \u001b[33m0:00:24\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m406 it/s\u001b[0m ]\n"
                },
                "metadata": {},
                "output_type": "display_data"
              }
            ]
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
