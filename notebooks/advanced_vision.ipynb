{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connectome-constrained visual system model\n",
    "\n",
    "**Authors:** Thomas Ka Chung Lam, Sibo Wang-Chen\n",
    "\n",
    "**Note:** The code presented in this notebook has been simplified and restructured for display in a notebook format. A more complete and better structured implementation can be found in the [examples folder of the FlyGym repository on GitHub](https://github.com/NeLy-EPFL/flygym/tree/main/flygym/examples/).\n",
    "\n",
    "**Summary**: In this tutorial, we will (1) simulate two flies in the same arena, and (2) integrate a connectome-constrained visual system model [(Lappalainen et al., 2023)](https://www.biorxiv.org/content/10.1101/2023.03.11.532232) into NeuroMechFly. Combining these, we will simulate a scenario where a stationary fly observes another fly walking in front of it and examine the responses of different neurons in the visual system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation with multiple flies\n",
    "\n",
    "In the previous tutorial, we have used `HybridTurningController` to control the walking fly at a high level of abstraction â€” instead of specifying joint movements, we provide a 2D descending drive and rely on the underlying CPG network and sensory feedback-based correction mechanism to compute the appropriate joint actions. Details of this controller was covered in [this tutorial](https://neuromechfly.org/tutorials/turning.html). One limitation of this design decision is that the controller is implemented at the level of the Simulation (recall that `HybridTurningController` extends the `Simulation` class). Therefore, it is difficult to model multiple flies, each driven by a hybrid turning controller, in a single simulation.\n",
    "\n",
    "To overcome this limitation, we can implement the core logic of the hybrid turning controller at the level of the Fly instead (see [API reference](https://neuromechfly.org/api_ref/examples/locomotion.html#hybrid-turning-fly)). Because we can have multiple Fly objects in the same Simulation, this approach allows us to separately control multiple `HybridTurningFly` instances. Let's spawn two flies: a target fly that walks forward and an observer fly that observes the target fly perpendicular to the target fly's direction of movement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "output_dir = Path(\"outputs/advanced_vision/\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from flygym import Camera, Simulation\n",
    "from flygym.examples.locomotion import HybridTurningFly\n",
    "\n",
    "timestep = 1e-4\n",
    "contact_sensor_placements = [\n",
    "    f\"{leg}{segment}\"\n",
    "    for leg in [\"LF\", \"LM\", \"LH\", \"RF\", \"RM\", \"RH\"]\n",
    "    for segment in [\"Tibia\", \"Tarsus1\", \"Tarsus2\", \"Tarsus3\", \"Tarsus4\", \"Tarsus5\"]\n",
    "]\n",
    "\n",
    "target_fly = HybridTurningFly(\n",
    "    name=\"target\",\n",
    "    enable_adhesion=True,\n",
    "    draw_adhesion=False,\n",
    "    contact_sensor_placements=contact_sensor_placements,\n",
    "    seed=0,\n",
    "    draw_corrections=True,\n",
    "    timestep=timestep,\n",
    "    spawn_pos=(3, 3, 0.5),\n",
    "    spawn_orientation=(0, 0, 0),\n",
    ")\n",
    "\n",
    "observer_fly = HybridTurningFly(\n",
    "    name=\"observer\",\n",
    "    enable_adhesion=True,\n",
    "    draw_adhesion=False,\n",
    "    enable_vision=True,\n",
    "    contact_sensor_placements=contact_sensor_placements,\n",
    "    seed=0,\n",
    "    draw_corrections=True,\n",
    "    timestep=timestep,\n",
    "    spawn_pos=(0, 0, 0.5),\n",
    "    spawn_orientation=(0, 0, np.pi / 2),\n",
    "    # setting head_stabilization_model to \"thorax\" will make actuate\n",
    "    # neck joints according to actual thorax rotations (i.e., using ideal\n",
    "    # head stabilization signals)\n",
    "    head_stabilization_model=\"thorax\",\n",
    ")\n",
    "\n",
    "flies = [observer_fly, target_fly]\n",
    "cam = Camera(attachment_point=observer_fly.model.worldbody,\n",
    "    camera_name=\"camera_top_zoomout\",\n",
    "    targeted_fly_names=observer_fly.name,\n",
    "    play_speed=0.1,\n",
    ")\n",
    "sim = Simulation(\n",
    "    flies=flies,\n",
    "    cameras=[cam],\n",
    "    timestep=timestep,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can control the movement of the flies separately by passing two sets of actions, indexed by the fly names. In this example, we will keep the observer fly stationary and make the target fly walk straight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "run_time = 0.5  # sec\n",
    "\n",
    "obs, info = sim.reset(seed=0)\n",
    "for i in trange(int(run_time / timestep)):\n",
    "    obs, _, _, _, info = sim.step(\n",
    "        {\n",
    "            \"observer\": np.zeros(2),  # stand still\n",
    "            \"target\": np.ones(2),  # walk forward\n",
    "        }\n",
    "    )\n",
    "    sim.render()\n",
    "\n",
    "cam.save_video(output_dir / \"two_flies_walking.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interfacing NeuroMechFly with a connectome-constrained vision model\n",
    "\n",
    "So far, we have implemented various abstract, algorithmic controllers to control a diverse range of behaviors in NeuroMechFly. Ultimately, to gain insights into the real workings of the biological controller, one would ideally build a controller with artificial neurons that can be mapped to neuron subtypes in the real fly nervous system. This can, in principle, be achieved by leveraging newly available brain and VNC connectomics datasets (see the [FlyWire](https://flywire.ai/) project for the brain, and the [FANC](https://connectomics.hms.harvard.edu/project1) and [MANC](https://www.janelia.org/project-team/flyem/manc-connectome) projects for the VNC).\n",
    "\n",
    "To illustrate how this might be accomplished, we will interface NeuroMechFly a recently established connectome-constrained neural network model ([Lappalainen et al., 2023](https://www.biorxiv.org/content/10.1101/2023.03.11.532232); [code](https://github.com/TuragaLab/flyvis)). This study has constructed an artificial neural network (ANN) representing the retina, lamina, medula, lobula plate, and lobula of the fly visual system (see figure below). The connectivity in this network is informed by the connectome and, unlike typical ANNs, models biologically meaningful variables such as voltage.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://github.com/NeLy-EPFL/_media/blob/main/flygym/advanced_vision/lappalainen_model_schematic.png?raw=true\" alt=\"lappalainen_model_schematic.png\" width=\"500\"/>\n",
    "</p>\n",
    "\n",
    "_Image from Lappalainen et al., 2023._\n",
    "\n",
    "We will pass the visual experience of the simulated fly as inputs to this pretrained model and simulate the activities of real neurons. For this purpose, we have implemented a `RealisticVisionFly` class that extends `HybridTurningFly`. Let's initialize the simulation but replace the observer fly with an instance of `RealisticVisionFly`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flygym.examples.vision import RealisticVisionFly\n",
    "\n",
    "target_fly = HybridTurningFly(\n",
    "    name=\"target\",\n",
    "    enable_adhesion=True,\n",
    "    draw_adhesion=False,\n",
    "    contact_sensor_placements=contact_sensor_placements,\n",
    "    seed=0,\n",
    "    draw_corrections=True,\n",
    "    timestep=timestep,\n",
    "    spawn_pos=(3, 3, 0.5),\n",
    "    spawn_orientation=(0, 0, 0),\n",
    ")\n",
    "\n",
    "observer_fly = RealisticVisionFly(\n",
    "    name=\"observer\",\n",
    "    spawn_pos=(0, 0, 0.5),\n",
    "    spawn_orientation=(0, 0, np.pi / 2),\n",
    "    contact_sensor_placements=contact_sensor_placements,\n",
    "    head_stabilization_model=\"thorax\",\n",
    ")\n",
    "\n",
    "flies = [observer_fly, target_fly]\n",
    "cam = Camera(attachment_point=observer_fly.model.worldbody,\n",
    "        camera_name=\"camera_top_zoomout\",\n",
    "        targeted_flies_id=[0],\n",
    "        play_speed=0.1,\n",
    "        )\n",
    "\n",
    "sim = Simulation(\n",
    "    flies=flies,\n",
    "    cameras=[cam],\n",
    "    timestep=timestep,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can implement the main simulation loop as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, info = sim.reset(seed=0)\n",
    "viz_data_all = []\n",
    "obs_hist = []\n",
    "info_hist = []\n",
    "\n",
    "for i in trange(int(run_time / timestep)):\n",
    "    obs, _, _, _, info = sim.step(\n",
    "        {\n",
    "            \"observer\": np.zeros(2),  # stand still\n",
    "            \"target\": np.ones(2),  # walk forward\n",
    "        }\n",
    "    )\n",
    "    obs_hist.append(obs)\n",
    "    info_hist.append(info)\n",
    "\n",
    "    rendered_img = sim.render()[0]\n",
    "\n",
    "    if rendered_img is not None:\n",
    "        viz_data = {\n",
    "            \"rendered_image\": rendered_img,\n",
    "            \"vision_observation\": obs[\"observer\"][\"vision\"],  # raw visual observation\n",
    "            \"nn_activities\": info[\"observer\"][\"nn_activities\"],  # neural activities\n",
    "        }\n",
    "        viz_data_all.append(viz_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the \"info\" dictionary, we can get the \"nn_activities\" entry, which is an extended dictionary containing the current activities of all neurons simulated in the network. For a complete definition of what the simulation returns in the observation and \"info\" dictionary, please refer to the [MDP Task Specification page](https://neuromechfly.org/api_ref/mdp_specs.html) of the API reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(info[\"observer\"][\"nn_activities\"].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, we can access the activities of the T4a/b/c/d neurons, which are known for encoding optical flow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(\n",
    "    1, 5, figsize=(6, 2), width_ratios=[2, 2, 2, 2, 0.2], tight_layout=True\n",
    ")\n",
    "\n",
    "for i, cell in enumerate([\"T4a\", \"T4b\", \"T4c\", \"T4d\"]):\n",
    "    ax = axs[i]\n",
    "\n",
    "    # Take the cell activities of the right eye (index 1)\n",
    "    cell_activities = info[\"observer\"][\"nn_activities\"][cell][1]\n",
    "    cell_activities = observer_fly.retina_mapper.flyvis_to_flygym(cell_activities)\n",
    "\n",
    "    # Convert the values of 721 cells to a 2D image\n",
    "    viz_img = observer_fly.retina.hex_pxls_to_human_readable(cell_activities)\n",
    "    viz_img[observer_fly.retina.ommatidia_id_map == 0] = np.nan\n",
    "    imshow_obj = ax.imshow(viz_img, cmap=\"seismic\", vmin=-2, vmax=2)\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_title(cell)\n",
    "\n",
    "cbar = plt.colorbar(\n",
    "    imshow_obj,\n",
    "    cax=axs[4],\n",
    ")\n",
    "fig.savefig(output_dir / \"retina_activities.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also extract the whole time series of cell activities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cell_activities = np.array(\n",
    "    [obs[\"observer\"][\"nn_activities_arr\"] for obs in obs_hist]\n",
    ")\n",
    "print(all_cell_activities.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... where the shape is (num_timesteps, num_eyes=2, num_cells_per_eye=45669).\n",
    "\n",
    "To visualize this block data better, we have implemented a `visualize_vision` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flygym.examples.vision.viz import visualize_vision\n",
    "\n",
    "plt.ioff()  # turn off interactive display of image\n",
    "visualize_vision(\n",
    "    video_path=output_dir / \"two_flies_walking_vision.mp4\",\n",
    "    retina=observer_fly.retina,\n",
    "    retina_mapper=observer_fly.retina_mapper,\n",
    "    viz_data_all=viz_data_all,\n",
    "    fps=cam.fps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flygym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
